{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Winning Jeopardy\n",
    "## Alex Haughton\n",
    "The goal of this project is to use existing questions from a Jeopardy game show dataset to maximize our odds of winning a game of jeopardy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Show Number', ' Air Date', ' Round', ' Category', ' Value',\n",
       "       ' Question', ' Answer'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Show Number</th>\n",
       "      <th>Air Date</th>\n",
       "      <th>Round</th>\n",
       "      <th>Category</th>\n",
       "      <th>Value</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>HISTORY</td>\n",
       "      <td>$200</td>\n",
       "      <td>For the last 8 years of his life, Galileo was ...</td>\n",
       "      <td>Copernicus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>ESPN's TOP 10 ALL-TIME ATHLETES</td>\n",
       "      <td>$200</td>\n",
       "      <td>No. 2: 1912 Olympian; football star at Carlisl...</td>\n",
       "      <td>Jim Thorpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>EVERYBODY TALKS ABOUT IT...</td>\n",
       "      <td>$200</td>\n",
       "      <td>The city of Yuma in this state has a record av...</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>THE COMPANY LINE</td>\n",
       "      <td>$200</td>\n",
       "      <td>In 1963, live on \"The Art Linkletter Show\", th...</td>\n",
       "      <td>McDonald's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>EPITAPHS &amp; TRIBUTES</td>\n",
       "      <td>$200</td>\n",
       "      <td>Signer of the Dec. of Indep., framer of the Co...</td>\n",
       "      <td>John Adams</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Show Number    Air Date      Round                         Category  Value  \\\n",
       "0         4680  2004-12-31  Jeopardy!                          HISTORY   $200   \n",
       "1         4680  2004-12-31  Jeopardy!  ESPN's TOP 10 ALL-TIME ATHLETES   $200   \n",
       "2         4680  2004-12-31  Jeopardy!      EVERYBODY TALKS ABOUT IT...   $200   \n",
       "3         4680  2004-12-31  Jeopardy!                 THE COMPANY LINE   $200   \n",
       "4         4680  2004-12-31  Jeopardy!              EPITAPHS & TRIBUTES   $200   \n",
       "\n",
       "                                            Question      Answer  \n",
       "0  For the last 8 years of his life, Galileo was ...  Copernicus  \n",
       "1  No. 2: 1912 Olympian; football star at Carlisl...  Jim Thorpe  \n",
       "2  The city of Yuma in this state has a record av...     Arizona  \n",
       "3  In 1963, live on \"The Art Linkletter Show\", th...  McDonald's  \n",
       "4  Signer of the Dec. of Indep., framer of the Co...  John Adams  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "import pandas as pd\n",
    "jeopardy = pd.read_csv('jeopardy.csv')\n",
    "jeopardy.columns\n",
    "jeopardy.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets put the columns in camel case and strip the leading spaces for good style practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ShowNumber', 'AirDate', 'Round', 'Category', 'Value', 'Question',\n",
       "       'Answer'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jeopardy.columns = jeopardy.columns.str.replace(' ','')\n",
    "jeopardy.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start to analyze the jeopardy questions and answers, we need to normalize all of the text in both columns. We'll accomplish this by putting the string in lowercase and removing all punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jeopardy['clean_answer']=jeopardy['Answer'].str.lower().str.replace('[\\'\\,\\.\\(\\)\\\";]','')\n",
    "jeopardy['clean_question']=jeopardy['Question'].str.lower().str.replace('[\\'\\,\\.\\(\\)\\\";]','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Value\" column should also be numeric in order to manipulate it more easily, and the \"Air Date\" should be in datetime format, not string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize_dollar_values(value):\n",
    "    clean_val = re.sub('[$,]','',value)\n",
    "    try:\n",
    "        return int(clean_val)\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "jeopardy['clean_value'] = jeopardy['Value'].apply(normalize_dollar_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Convert str to datetime format\n",
    "if type(jeopardy['AirDate'][0]) == str:\n",
    "    jeopardy['AirDate'] = pd.to_datetime(jeopardy['AirDate'],format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is ultimately to determine whether it is best to study specific past questions, general knowledge, or not study it at all. Two factors will be important: how often an answer is deducible from the question, and how often new quesitons are repeats of older questions.\n",
    "\n",
    "To answer the first question, we'll determine how many question/answer pairs have complex words (>6 characters) common to both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def deducible(row):\n",
    "    split_answer = row[0].split()\n",
    "    \n",
    "    #Remove particles\n",
    "    split_answer = [word for word in split_answer if word not in ['a','an','the']]\n",
    "    split_question = row[1].split()\n",
    "    \n",
    "    #If has no words now, return 0 to avoid division by zero error\n",
    "    if len(split_answer) == 0:\n",
    "        return 0 \n",
    "    \n",
    "    #Count how many of the words in the answer are found in the question\n",
    "    match_count = 0\n",
    "    for word in split_answer:\n",
    "        if word in split_question:\n",
    "            match_count += 1\n",
    "            \n",
    "    return match_count/len(split_answer)\n",
    "\n",
    "jeopardy['answer_in_question'] = (jeopardy[['clean_answer','clean_question']]\n",
    "                                  .apply(deducible,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ShowNumber</th>\n",
       "      <th>AirDate</th>\n",
       "      <th>Round</th>\n",
       "      <th>Category</th>\n",
       "      <th>Value</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>clean_answer</th>\n",
       "      <th>clean_question</th>\n",
       "      <th>clean_value</th>\n",
       "      <th>answer_in_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10556</th>\n",
       "      <td>5281</td>\n",
       "      <td>2007-07-23</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>THE LARGEST IN AREA</td>\n",
       "      <td>$1000</td>\n",
       "      <td>Libya, Egypt, Tunisia</td>\n",
       "      <td>Libya</td>\n",
       "      <td>libya</td>\n",
       "      <td>libya egypt tunisia</td>\n",
       "      <td>1000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18064</th>\n",
       "      <td>3227</td>\n",
       "      <td>1998-09-22</td>\n",
       "      <td>Double Jeopardy!</td>\n",
       "      <td>PUT 'EM IN ORDER</td>\n",
       "      <td>$400</td>\n",
       "      <td>Calamity Jane, Jane Curtin, Lady Jane Grey</td>\n",
       "      <td>Lady Jane Grey, Calamity Jane, Jane Curtin</td>\n",
       "      <td>lady jane grey calamity jane jane curtin</td>\n",
       "      <td>calamity jane jane curtin lady jane grey</td>\n",
       "      <td>400</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3225</th>\n",
       "      <td>5084</td>\n",
       "      <td>2006-10-19</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>THE HIGHEST-SCORING SCRABBLE WORD</td>\n",
       "      <td>$200</td>\n",
       "      <td>Hell, heaven or limbo</td>\n",
       "      <td>heaven</td>\n",
       "      <td>heaven</td>\n",
       "      <td>hell heaven or limbo</td>\n",
       "      <td>200</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ShowNumber    AirDate             Round  \\\n",
       "10556        5281 2007-07-23         Jeopardy!   \n",
       "18064        3227 1998-09-22  Double Jeopardy!   \n",
       "3225         5084 2006-10-19         Jeopardy!   \n",
       "\n",
       "                                Category  Value  \\\n",
       "10556                THE LARGEST IN AREA  $1000   \n",
       "18064                   PUT 'EM IN ORDER   $400   \n",
       "3225   THE HIGHEST-SCORING SCRABBLE WORD   $200   \n",
       "\n",
       "                                         Question  \\\n",
       "10556                       Libya, Egypt, Tunisia   \n",
       "18064  Calamity Jane, Jane Curtin, Lady Jane Grey   \n",
       "3225                        Hell, heaven or limbo   \n",
       "\n",
       "                                           Answer  \\\n",
       "10556                                       Libya   \n",
       "18064  Lady Jane Grey, Calamity Jane, Jane Curtin   \n",
       "3225                                       heaven   \n",
       "\n",
       "                                   clean_answer  \\\n",
       "10556                                     libya   \n",
       "18064  lady jane grey calamity jane jane curtin   \n",
       "3225                                     heaven   \n",
       "\n",
       "                                 clean_question  clean_value  \\\n",
       "10556                       libya egypt tunisia         1000   \n",
       "18064  calamity jane jane curtin lady jane grey          400   \n",
       "3225                       hell heaven or limbo          200   \n",
       "\n",
       "       answer_in_question  \n",
       "10556                 1.0  \n",
       "18064                 1.0  \n",
       "3225                  1.0  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jeopardy.sort_values('answer_in_question',ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the questions where all the words in the answer are found in the question ('answer_in_question' == 1), it appears that some jeopardy questions (and whole categories) are sometimes multiple choice, where the contestants choose one of several answers provided in the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.043594567618268805"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.000000    18134\n",
       "0.500000      978\n",
       "0.333333      389\n",
       "0.250000      131\n",
       "1.000000      125\n",
       "0.666667       72\n",
       "0.200000       67\n",
       "0.400000       24\n",
       "0.166667       22\n",
       "0.142857       13\n",
       "0.750000       11\n",
       "0.125000        7\n",
       "0.285714        6\n",
       "0.600000        4\n",
       "0.300000        2\n",
       "0.100000        2\n",
       "0.111111        2\n",
       "0.800000        2\n",
       "0.428571        2\n",
       "0.153846        1\n",
       "0.222222        1\n",
       "0.307692        1\n",
       "0.272727        1\n",
       "0.375000        1\n",
       "0.857143        1\n",
       "Name: answer_in_question, dtype: int64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.09325466273313665"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jeopardy['answer_in_question'].mean()\n",
    "jeopardy['answer_in_question'].value_counts().sort_values(ascending=False)\n",
    "answer_in_question = [1 if val > 0 else 0 for val in jeopardy['answer_in_question']]\n",
    "sum(answer_in_question)/len(answer_in_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Overall, trying to deduce the answer based on the question doesn't seem like a good strategy, fewer than 10% of the questions have even one word of the answer contained in the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll check how often questions in this dataset are repeats of older questions. We'll check not by looking for verbatim repeats of questions, but by looking for repeated usage of terms (6 letters or longer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "question_overlap = []\n",
    "terms_used = set()\n",
    "\n",
    "for index, row in jeopardy.sort_values('AirDate').iterrows():\n",
    "    \n",
    "    #Get terms 6 letters or longer from each question\n",
    "    split_question = row['clean_question'].split()\n",
    "    split_question = [word for word in split_question if len(word)>5]\n",
    "    match_count = 0\n",
    "    \n",
    "    #If term not used yet, add to set. If already used, counts as a match\n",
    "    for term in split_question:\n",
    "        if term in terms_used:\n",
    "            match_count += 1\n",
    "        else:\n",
    "            terms_used.add(term)\n",
    "            \n",
    "    #Return matches as fraction of total >5 letter terms in question\n",
    "    if len(split_question) > 0:\n",
    "        question_overlap.append(match_count/len(split_question))\n",
    "    else:\n",
    "        question_overlap.append(0)\n",
    "        \n",
    "#Add resulting list to dataframe\n",
    "jeopardy['question_overlap'] = question_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6799437402867058"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jeopardy['question_overlap'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, it appears that nearly 68% of the terms in the questions for this dataset are repeats of past questions. This indicates that studying past questions is probably a good strategy for answering questions asked in future games.\n",
    "\n",
    "To win Jeopardy, it's important to focus more on high value questions than low, as we will score more points for answering the same number of questions correctly. One way to focus our studying is to look for terms which come up more frequently in high value questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 3), (1, 1), (10, 13), (0, 1), (0, 1), (0, 1), (0, 2), (1, 1), (0, 1)]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Classify each question as high or low value\n",
    "jeopardy['high_value']=[1 if value >= 800 else 0 for value in jeopardy['clean_value']]\n",
    "\n",
    "def word_value(word):\n",
    "    low_count = 0\n",
    "    high_count = 0\n",
    "    \n",
    "    #Count number of times word occurs in high and low value questions\n",
    "    for index, row in jeopardy.iterrows():\n",
    "        if word in row['clean_question'].split():\n",
    "            if row['high_value'] == 1:\n",
    "                high_count += 1\n",
    "            else:\n",
    "                low_count += 1\n",
    "    return high_count, low_count\n",
    "\n",
    "observed_expected = []\n",
    "\n",
    "#Convert set from earlier into list\n",
    "comparison_terms = list(terms_used)[21:30] \n",
    "\n",
    "for term in comparison_terms:\n",
    "    observed_expected.append(word_value(term))\n",
    "    \n",
    "observed_expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way is straightforward, but is very inefficient and cannot find the frequency for more than a few terms in a reasonable time. Lets try to write a more efficient algorithm to check for the frequency of terms in high and low value questions by avoiding nested loops and taking advantage of pandas string methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "observed_high = []\n",
    "observed_low = [] \n",
    "\n",
    "list_terms = list(terms_used)\n",
    "\n",
    "#Add whitespace to tail and end of question so that first and last words are counted\n",
    "jeopardy['clean_question'] = ' '+jeopardy['clean_question']+' '\n",
    "\n",
    "for term in list_terms[0:1000]:\n",
    "    #Add whitespace to term to avoid counting terms contained in other words\n",
    "    contains_term = jeopardy['clean_question'].str.contains(' '+term+' ')\n",
    "    \n",
    "    #Count questions which are high/low value and contain the term\n",
    "    observed_high.append((contains_term & jeopardy['high_value']==1).sum())\n",
    "    observed_low.append((contains_term & (jeopardy['high_value']==0)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>observed_high</th>\n",
       "      <th>observed_low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>breton</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>elberon</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>buhner</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>clevelands</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jawless</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>href=http://wwwj-archivecom/media/2009-05-04_d...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>battlefield</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hypertension</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>longing</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>find--</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>clarkson</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>radiant</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>begley</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>thither</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>doctrine</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>vespiary</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>chanteuses</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>kennedys</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>canned</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>cheddar</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>shiitakes</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>temperatures</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>balanchine</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>players</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>lugging</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>clayderman</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>shipleys</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>dinners</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>schweitzer</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>picassoesque</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>pragues</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>spurned</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>bullfighting</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>offensive</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>heresy</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>eastman</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>fleecy</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>hartebeests</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>acasta</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>wagonmaker</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>representative</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>squadron</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>dependents</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>bernstein</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>reptilian</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>deaconess</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>jurors</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>entree</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>emotional</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>sticks&lt;/a&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>annette</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>cubists</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>foisted</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>genuine</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>spielberg</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>injections</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>alonzo</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>attached</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>minoxidil</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>projection</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  term  observed_high  \\\n",
       "0                                               breton              1   \n",
       "1                                              elberon              1   \n",
       "2                                               buhner              0   \n",
       "3                                           clevelands              2   \n",
       "4                                              jawless              1   \n",
       "5    href=http://wwwj-archivecom/media/2009-05-04_d...              1   \n",
       "6                                          battlefield              3   \n",
       "7                                         hypertension              0   \n",
       "8                                              longing              0   \n",
       "9                                               find--              1   \n",
       "10                                            clarkson              1   \n",
       "11                                             radiant              1   \n",
       "12                                              begley              1   \n",
       "13                                             thither              0   \n",
       "14                                            doctrine              1   \n",
       "15                                            vespiary              1   \n",
       "16                                          chanteuses              1   \n",
       "17                                            kennedys              1   \n",
       "18                                              canned              1   \n",
       "19                                             cheddar              1   \n",
       "20                                           shiitakes              1   \n",
       "21                                        temperatures              3   \n",
       "22                                          balanchine              1   \n",
       "23                                             players             10   \n",
       "24                                             lugging              0   \n",
       "25                                          clayderman              0   \n",
       "26                                            shipleys              0   \n",
       "27                                             dinners              0   \n",
       "28                                          schweitzer              1   \n",
       "29                                        picassoesque              0   \n",
       "..                                                 ...            ...   \n",
       "970                                            pragues              1   \n",
       "971                                            spurned              0   \n",
       "972                                       bullfighting              0   \n",
       "973                                          offensive              3   \n",
       "974                                             heresy              0   \n",
       "975                                            eastman              1   \n",
       "976                                             fleecy              1   \n",
       "977                                        hartebeests              0   \n",
       "978                                             acasta              1   \n",
       "979                                         wagonmaker              1   \n",
       "980                                     representative              2   \n",
       "981                                           squadron              1   \n",
       "982                                         dependents              0   \n",
       "983                                          bernstein              2   \n",
       "984                                          reptilian              1   \n",
       "985                                          deaconess              0   \n",
       "986                                             jurors              2   \n",
       "987                                             entree              1   \n",
       "988                                          emotional              1   \n",
       "989                                         sticks</a>              0   \n",
       "990                                            annette              1   \n",
       "991                                            cubists              0   \n",
       "992                                            foisted              1   \n",
       "993                                            genuine              0   \n",
       "994                                          spielberg              1   \n",
       "995                                         injections              2   \n",
       "996                                             alonzo              0   \n",
       "997                                           attached              5   \n",
       "998                                          minoxidil              0   \n",
       "999                                         projection              4   \n",
       "\n",
       "     observed_low  \n",
       "0               1  \n",
       "1               0  \n",
       "2               1  \n",
       "3               0  \n",
       "4               0  \n",
       "5               0  \n",
       "6               0  \n",
       "7               1  \n",
       "8               1  \n",
       "9               0  \n",
       "10              1  \n",
       "11              1  \n",
       "12              0  \n",
       "13              1  \n",
       "14              1  \n",
       "15              0  \n",
       "16              0  \n",
       "17              3  \n",
       "18              0  \n",
       "19              2  \n",
       "20              0  \n",
       "21              3  \n",
       "22              1  \n",
       "23             13  \n",
       "24              1  \n",
       "25              1  \n",
       "26              1  \n",
       "27              2  \n",
       "28              1  \n",
       "29              1  \n",
       "..            ...  \n",
       "970             0  \n",
       "971             1  \n",
       "972             1  \n",
       "973             3  \n",
       "974             1  \n",
       "975             1  \n",
       "976             0  \n",
       "977             1  \n",
       "978             0  \n",
       "979             0  \n",
       "980             5  \n",
       "981             1  \n",
       "982             1  \n",
       "983             5  \n",
       "984             0  \n",
       "985             1  \n",
       "986             1  \n",
       "987             0  \n",
       "988             1  \n",
       "989             2  \n",
       "990             0  \n",
       "991             1  \n",
       "992             0  \n",
       "993             1  \n",
       "994             5  \n",
       "995             0  \n",
       "996             1  \n",
       "997             6  \n",
       "998             1  \n",
       "999             0  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [('term',list_terms[0:1000]),\n",
    "        ('observed_high',observed_high),\n",
    "        ('observed_low',observed_low)]\n",
    "terms_df = pd.DataFrame.from_items(data)\n",
    "terms_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets test our newly created terms_df against results from our slow function to see if they match up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results from 30 random samples match with the old formula\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "trys = 30\n",
    "for test in range(0,trys):\n",
    "    test_row = terms_df.sample(n=1).iloc[0]\n",
    "    test_term, test_observed_high, test_observed_low = test_row\n",
    "    test_observed = (test_observed_high,test_observed_low)\n",
    "    results.append(word_value(test_term)==test_observed)\n",
    "\n",
    "if pd.Series(results).all():\n",
    "    print('The results from {} random samples match with the old formula'.format(trys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From 30 random samples, we get the same results using our more efficient algorithm. We will assume this algorithm is accurate enough to get the frequency for any term in our list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The function seems to work, now lets find the expected counts for each word in the term list so we can determine which terms come up significantly more in high value questions. For now, we'll try with 1000 of the 25,000 terms found in the answers and see if we get any useful information that justifies extending the algorithm to the entire terms dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dataquest/system/env/python3/lib/python3.4/site-packages/scipy/stats/stats.py:4350: RuntimeWarning: invalid value encountered in true_divide\n",
      "  terms = (f_obs - f_exp)**2 / f_exp\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import chisquare\n",
    "\n",
    "#Total number of high and low value numbers\n",
    "high_value_count = sum(jeopardy['high_value']==1)\n",
    "low_value_count = sum(jeopardy['high_value']==0)\n",
    "\n",
    "chi_squared = []\n",
    "chi_prob = []\n",
    "\n",
    "for index, row in terms_df.iterrows():\n",
    "    #Find expected number of high and low value questions containing term\n",
    "    observed = (row['observed_high'],row['observed_low'])\n",
    "    total = sum(observed)\n",
    "    total_prop = total/jeopardy.shape[0]\n",
    "    expected_high = total_prop*high_value_count\n",
    "    expected_low = total_prop*low_value_count\n",
    "    expected = (expected_high, expected_low)\n",
    "    \n",
    "    #Find chi squared value for each\n",
    "    chisq, p = chisquare(observed,expected)  \n",
    "    chi_squared.append(chisq)\n",
    "    chi_prob.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>observed_high</th>\n",
       "      <th>observed_low</th>\n",
       "      <th>chisq</th>\n",
       "      <th>chisq_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>battlefield</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3.885127</td>\n",
       "      <td>0.048716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>egypt&lt;/a&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3.858039</td>\n",
       "      <td>0.049508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>mariners</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3.885127</td>\n",
       "      <td>0.048716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>contiguous</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3.885127</td>\n",
       "      <td>0.048716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>prepares</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3.885127</td>\n",
       "      <td>0.048716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>target=_blank&gt;seen</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3.885127</td>\n",
       "      <td>0.048716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>bosworth</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3.885127</td>\n",
       "      <td>0.048716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>lab&lt;/a&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3.885127</td>\n",
       "      <td>0.048716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>thrillers</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3.885127</td>\n",
       "      <td>0.048716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>[gasps]</td>\n",
       "      <td>3024</td>\n",
       "      <td>3586</td>\n",
       "      <td>12.737693</td>\n",
       "      <td>0.000358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>resort</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>4.771605</td>\n",
       "      <td>0.028933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>forked</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3.885127</td>\n",
       "      <td>0.048716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>mammoth</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3.885127</td>\n",
       "      <td>0.048716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>servant</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>4.282865</td>\n",
       "      <td>0.038498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>susanna</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3.860877</td>\n",
       "      <td>0.049424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>sketch</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5.180170</td>\n",
       "      <td>0.022846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>satire</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5.180170</td>\n",
       "      <td>0.022846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>earths</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>6.009253</td>\n",
       "      <td>0.014231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>perfect</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>4.555472</td>\n",
       "      <td>0.032814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>neeson</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3.885127</td>\n",
       "      <td>0.048716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>projection</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5.180170</td>\n",
       "      <td>0.022846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   term  observed_high  observed_low      chisq  chisq_prob\n",
       "6           battlefield              3             0   3.885127    0.048716\n",
       "145           egypt</a>              5             1   3.858039    0.049508\n",
       "160            mariners              3             0   3.885127    0.048716\n",
       "168          contiguous              3             0   3.885127    0.048716\n",
       "274            prepares              3             0   3.885127    0.048716\n",
       "382  target=_blank>seen              3             0   3.885127    0.048716\n",
       "388            bosworth              3             0   3.885127    0.048716\n",
       "422             lab</a>              3             0   3.885127    0.048716\n",
       "430           thrillers              3             0   3.885127    0.048716\n",
       "438             [gasps]           3024          3586  12.737693    0.000358\n",
       "477              resort             13             6   4.771605    0.028933\n",
       "536              forked              3             0   3.885127    0.048716\n",
       "639             mammoth              3             0   3.885127    0.048716\n",
       "648             servant              7             2   4.282865    0.038498\n",
       "670             susanna              0             5   3.860877    0.049424\n",
       "757              sketch              4             0   5.180170    0.022846\n",
       "760              satire              4             0   5.180170    0.022846\n",
       "798              earths             13             5   6.009253    0.014231\n",
       "858             perfect             14             7   4.555472    0.032814\n",
       "882              neeson              3             0   3.885127    0.048716\n",
       "999          projection              4             0   5.180170    0.022846"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms_df['chisq']=chi_squared\n",
    "terms_df['chisq_prob']=chi_prob\n",
    "\n",
    "terms_df[terms_df['chisq_prob']<0.05]\n",
    "(terms_df['chisq_prob']<0.05).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've successfully found a set of words that are overrepresented in the high or low value questions. Unfortunately, there are a few issues with this result. For one, knowing one word that occurs in the question doesn't tell us much about how to study for the actual question. Knowing that a question contains the word \"perfect\" doesn't tell us a thing about the rest of the question. Two, from 1000 questions, we were only able to find 21 terms which were overrepresented in high or low value questions, which suggests there isn't a strong correlation between the words in a question and the value of the question. Three, there are \"nonsense\" terms in our terms list which need to be filtered out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, from our investigation, we still don't have an obvious studying strategy to follow outside of studying past questions, as we found that the majority of questions asked in Jeopardy are similar to questions asked previously. We cannot make any recommendations for specific topics to study based on our term chi-squared testing, as there are too few terms significantly overrepresented in high value questions, and the ones that we found are not helpful in determining the nature of the question or how to study for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One potential pattern which may be useful for future exploration is in looking at the categories of questions and more narrowly classifying them as history, science, mathematics, geography etc. This could give us an idea of which types of questions are most frequently asked, and we could focus our studying more narrowls on those topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
